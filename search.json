[
  {
    "objectID": "docs/dynamicid-sol.html",
    "href": "docs/dynamicid-sol.html",
    "title": "Identification in Dynamic Discrete Models: Solutions",
    "section": "",
    "text": "Simulation and Estimation\n\n\n\n\n\n\nProblem 1\n\n\n\nAdapt the equilibrium calculation, simulation, and estimation code in dyanmicgame.jl to compute an equlibrium, simulate, and estimate with a single agent firm (N=1). Most of the code will just work, except the transition function, but it is not needed here. Check your code by simulating some data with N=1 and Nexternal=2, and then estimating the model with the simulated data. Make a table similar to one in dynamicgame.jl at the end of the “Estimation” section comparing the true payoffs and estimated payoffs.\n\n\n\nEqulibrium and Simulation Code\nThis is unmodifed from the provided Pluto notebook.\n\nmodule DG\n\nusing NLsolve, LinearAlgebra, Distributions\n\nexport DynamicGame, equilibrium, Λ, vᵖ\n\nstruct DynamicGame{I&lt;:Integer, U&lt;:Function, R&lt;:Real, E&lt;:Function,A,S}\n    \"N is the number of players\"\n    N::I\n\n    \"\"\"\n    u(i, a, x) is the flow payoff function, where\n    - i is a player index\n    - a is a vector of actions of length N\n    - x is a state\n    \"\"\"\n    u::U\n\n    \"Discount factor\"\n    β::R\n\n    \"\"\"\n    A function returning a vector with Ex(a,x)[x̃] = P(x̃|a,x)\n    \"\"\"\n    Ex::E\n\n    \"\"\"\n    Set of actions, must be 1:A or similar.\n    \"\"\"\n    actions::A\n\n    \"\"\"\n    Set of states, must be 1:S or similar.\n    \"\"\"\n    states::S\nend\n\nfunction pmax(v::AbstractVector)\n    m = maximum(v)\n    p = exp.(v .- m)\n    p ./= sum(p)\n    return(p)\nend\n\nfunction emax(v::AbstractVector)\n    m = maximum(v)\n    return(Base.MathConstants.γ + m + log(sum(exp.(v .- m))))\nend\n\n\"\"\"\n   vᵖ(g::DynamicGame, P)\n\nComputes choice specific value functions given strategies P.\n\nP should be an array with P[i,a,x] = P(aᵢ=a| x)\n\"\"\"\nfunction vᵖ(g::DynamicGame, P)\n    v = similar(P)\n    a_all = Vector{eltype(g.actions)}(undef, g.N)\n    E = zeros(eltype(P), length(g.actions), length(g.states),\n        length(g.actions), length(g.states))\n    y = zeros(eltype(v), length(g.actions), length(g.states))\n    for i in 1:g.N\n        y .= zero(eltype(y))\n        E .= zero(eltype(E))\n        for a0 in g.actions\n            a_all[i] = a0\n            for x in g.states\n                for ami in Iterators.product(ntuple(i-&gt;g.actions, g.N-1)...)\n                    Pmi = one(eltype(P))\n                    for j in 1:g.N\n                        if j ≠ i\n                            k = j &lt; i ? j : (j-1)\n                            Pmi *= P[j, ami[k], x]\n                            a_all[j] = ami[k]\n                        end\n                    end\n                    y[a0,x] += Pmi*g.u(i,a_all,x)\n                    for an in g.actions\n                        E[a0,x,an,:] .+= Pmi*g.Ex(a_all,x).*P[i,an,:]\n                        y[a0,x] +=  Pmi*g.β * (g.Ex(a_all,x).*P[i,an,:])'*\n                            (-log.(P[i,an,:]) .+ Base.MathConstants.γ)\n                    end\n                end # ami\n            end # x\n        end # an\n        SA = length(g.states)*length(g.actions)\n        v[i, :, : ] .= reshape( (I - g.β*reshape(E, SA, SA)) \\ reshape(y, SA),\n                               length(g.actions), length(g.states))\n    end # i\n    return(v)\nend\n\n\"\"\"\n    V̄(g::DynamicGame, vᵖ)\n\nReturns the value function for game `g` with choice specific value functions vᵖ.\n\"\"\"\nfunction V̄(g::DynamicGame, vᵖ)\n    return([emax(vᵖ[i,:,x]) for i in 1:g.N, x in g.states])\nend\n\n\"\"\"\n    Λ(g::DynamicGame, vᵖ)\n\nComputes best response choice probabilities given choice specific value function.\n\"\"\"\nfunction Λ(g::DynamicGame, vᵖ)\n    p = similar(vᵖ)\n    for (i,x) in Iterators.product(1:g.N, g.states)\n        p[i,:,x] .= pmax(vᵖ[i,:,x])\n    end\n    return(p)\nend\n\n\n\"\"\"\n    equilibrium(g::DynamicGame)\n\nCompute equilibriumn choice probabilites of game `g`.\n\nReturns a tuple `(out, P)` where `out` is the return value of `nlsolve`, and the choice probabilities are `P`.\n\"\"\"\nfunction equilibrium(g::DynamicGame)\n    p = zeros(g.N,length(g.actions), length(g.states))\n    #p = rand(size(p)...)\n    #p .= 1/length(g.actions)\n    p[:,1,:] .= 0.1\n    p[:,2,:] .= 0.9\n    function probs(z)\n        p = similar(z, size(z,1), size(z,2) + 1, size(z,3))\n        ez = exp.(z)\n        for i in 1:size(p,1)\n            for x in 1:size(p,3)\n                se = sum(ez[i,:,x])\n                p[i,2:end,x] .= ez[i,:,x]./(1 + se)\n                p[i,1,x] = 1/(1 + se)\n            end\n        end\n        return(p)\n    end\n    z = log.(p[:,2:end,:])\n    for c in 1:size(z,2)\n        z[:,c,:] .-= log.(p[:,1,:])\n    end\n    function eq!(e,z)\n        p = probs(z)\n        e .= (p - Λ(g, vᵖ(g, p)))[:,2:end,:]\n        return(e)\n    end\n    out = nlsolve(eq!, z, autodiff=:forward, method=:trust_region)\n    return(out, probs(out.zero))\nend\n\n\"\"\"\n    simulate(g::DynamicGame, T, P; burnin=T, x0=rand(g.states))\n\nSimulates game `g` for `T` periods with strategies `P`. Begins from state `x0` and discards the first `burnin + 1` periods.\n\"\"\"\nfunction simulate(g::DynamicGame, T, P; burnin=T, x0=rand(g.states))\n    A = similar(g.actions,g.N,T)\n    U = zeros(g.N,T)\n    EV = copy(U)\n    V = copy(U)\n    X = similar(g.states,T)\n    x = copy(x0)\n    a = similar(g.actions, g.N)\n    v = vᵖ(g, P)\n    for t=-burnin:T\n        ϵ = rand(Gumbel(0,1),g.N,length(g.actions))\n        for i in 1:g.N\n            #(_, aold) = findmax(v[i,:,x] + ϵ[i,:])\n            (_, a[i]) = findmax(log.(P[i,:,x]).-log(P[i,1,x]) + ϵ[i,:])\n            #if (i &lt; 10)\n            #   a[i]==aold || error(\"uh oh\")\n            #end\n        end\n        if (t&gt;0)\n            A[:,t] .= a\n            X[t] = x\n            for i in 1:g.N\n                u = g.u(i,a,x)\n                U[i,t] = u + ϵ[i,a[i]]\n                V[i,t] = v[i,a[i],x] + ϵ[i,a[i]]\n                EV[i,t] =v[i,a[i],x] - log.(P[i,a[i],x]) + Base.MathConstants.γ\n            end\n        end\n        x = rand(DiscreteNonParametric(g.states, g.Ex(a,x)))\n    end\n    return(a=A, x=X, u=U, v=V, ev=EV)\nend\n\nend\n\nMain.Notebook.DG\n\n\n\n\nEstimation Code\n\n\"Dynamic game estimation.\"\nmodule DGE\n\nusing LinearAlgebra,  Statistics, Distributions\n\nfunction choiceprob(data)\n    states = sort(unique(data.x))\n    actions = sort(unique(data.a))\n    N = size(data.a,1)\n    P = [sum( (data.a[i,:].==a) .& (data.x.==x)) /\n            sum(data.x.==x) for i ∈ 1:N, a ∈ actions, x ∈ states]\n    P[isnan.(P)] .= 1/length(actions)\n    return P\nend\n\nfunction transitioni(data)\n    # P(x'|a_i,x,i)\n    states = sort(unique(data.x))\n    actions = sort(unique(data.a))\n    N  = size(data.a,1)\n    Pxi = [sum((data.x[2:end].==x̃) .& (data.x[1:(end-1)].==x)\n               .& (data.a[i,1:(end-1)].==a)) /\n           sum((data.x[1:(end-1)].==x) .& (data.a[i,1:(end-1)].==a) )\n           for i ∈ 1:N, x̃ ∈ states, a ∈ actions, x ∈ states]\n    Pxi[isnan.(Pxi)] .= 1/length(states)\n    return(Pxi)\nend\n\nfunction constructu(data, β; P=choiceprob(data), Pxi=transitioni(data), a0=1)\n    states = sort(unique(data.x))\n    actions = sort(unique(data.a))\n    N  = size(data.a,1)\n    Eu = similar(P)\n    Eu[:,a0,:] .= 0\n    v = similar(P)\n\n    # recover v[:,a0,:]\n    for i ∈ 1:N\n        q = [Base.MathConstants.γ - log(P[i,a0,x]) for x ∈ states]\n        E = Pxi[i,:,a0,:]'\n        y = Eu[i,a0,:] + β*E*q\n        v[i,a0,:] .= (I - β*E) \\ y\n\n        for a ∈ actions\n            if a ≠ a0\n                v[i,a,:] .= log.(P[i,a,:]) .- log.(P[i,a0,:]) .+ v[i,a0,:]\n            end\n        end\n        # recover E[u(a[i],a[-i],x)|a[i],x]\n        q = [Base.MathConstants.γ + log(sum(exp.(v[i,:,x]))) for x ∈ states]\n        for a ∈ actions\n            E = Pxi[i,:,a,:]'\n            Eu[i,a,:] .= v[i,a,:] .- β*E*q\n        end\n    end\n    return(Eu=Eu, v=v)\nend\n\n\nfunction transition(data)\n    states = sort(unique(data.x))\n    actions = sort(unique(data.a))\n    N  = size(data.a,1)\n    if N==1\n        @info \"N=1\"\n        Px = [sum( (data.x[2:end] .== x̃) .&\n            (data.a[1,1:(end-1)].==a1) .&\n            (data.x[1:(end-1)].==x)) /\n            sum((data.a[1,1:(end-1)].==a1) .&\n            (data.x[1:(end-1)].==x)) for x̃ ∈ states,\n                a1 ∈ actions, x ∈ states ]\n    elseif N==2\n        Px = [sum( (data.x[2:end] .== x̃) .&\n            (data.a[1,1:(end-1)].==a1) .&\n            (data.a[2,1:(end-1)].==a2) .&\n            (data.x[1:(end-1)].==x)) /\n            sum((data.a[1,1:(end-1)].==a1) .&\n            (data.a[2,1:(end-1)].==a2) .&\n            (data.x[1:(end-1)].==x)) for x̃ ∈ states,\n                a1 ∈ actions, a2 ∈ actions, x ∈ states ]\n    else\n        error(\"transition assumes 1 or 2 players\")\n    end\n    Px[isnan.(Px)] .= 1 ./ length(states)\n    return(Px)\nend\n\n\nfunction markovbootstrap(data, P = choiceprobs(data), Px=transition(data))\n    states = sort(unique(data.x))\n    actions = sort(unique(data.a))\n    N  = size(data.a,1)\n    bd = deepcopy(data)\n    T = length(data.x)\n    for t ∈ 2:T\n        bd.x[t] = rand(DiscreteNonParametric(states, Px[:,bd.a[:,t-1]...,bd.x[t-1]]))\n        for i in 1:N\n            bd.a[i,t] = rand(DiscreteNonParametric(actions,P[i,:,bd.x[t]]))\n        end\n    end\n    return(bd)\nend\n\nend\n\nMain.Notebook.DGE\n\n\n\n\nSetup and Equlibrium Computation\nDefine the payoff function and transition process for the external states.\n\n(N, ns, u, Ex, statevec, stateind, states) = let\n    N = 1\n    Nexternal=2\n\n    # There's often some tedious book keeping involved in going from an integer state index to a vector representation of a state\n    states = BitVector.(digits.(0:(2^(N+Nexternal)-1), base=2, pad=N+Nexternal))\n    statevec(x::Integer)=states[x]\n    stateind(s::AbstractVector)=findfirst([s==st for st in states])\n    u(i, a, x::Integer) = u(i,a,statevec(x))\n    function u(i, a, s::AbstractVector)\n        return( (a[i]-1)*s[i]*(3-Nexternal/2 + sum(s[(N+1):end]) - sum(s[1:N]))\n            - 0.5*(a[i]-1)*(1-s[i]) # entry cost\n            - s[i]*(0.8 + 0.1*sum(s[1:N]))*(a[i]-1)) # fixed cost\n    end\n\n    Ex(a, x::Integer) = Ex(a, statevec(x))\n    pstay = 0.7 # each binary external state stays the same with probability pstay\n    function Ex(a, s::AbstractVector)\n        E = zeros(length(states))\n        sn = copy(s)\n        sn[1:N] .= a.-1\n        for j in 0:(2^Nexternal-1)\n            sn[(N+1):end] .= digits(j, base=2, pad=Nexternal)\n            i = stateind(sn)\n            nsame = sum(sn[(N+1):end].==s[(N+1):end])\n            E[i] = pstay^(nsame)*(1-pstay)^(Nexternal-nsame)\n        end\n        return(E)\n    end\n    N, length(states), u, Ex, statevec, stateind, states\nend\n\n(1, 8, u, Ex, statevec, stateind, BitVector[[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 1]])\n\n\nCompute the equilibrium.\n\ng = DG.DynamicGame(N, u, 0.9, Ex, 1:2, 1:ns)\nres, choicep = DG.equilibrium(g)\n\n(Results of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219]\n * Zero: [0.3490566287664954;;; 0.9490566287664999;;; 0.6865563547520088;;; 2.286556354752003;;; 0.6865563547520055;;; 2.2865563547520065;;; 1.044137106305149;;; 3.6441371062256076]\n * Inf-norm of residuals: 0.000000\n * Iterations: 6\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 7\n * Jacobian Calls (df/dx): 6, [0.41361120482686586 0.586388795173134;;; 0.2790745811212805 0.7209254188787195;;; 0.3347995665976892 0.6652004334023108;;; 0.09224249488323547 0.9077575051167646;;; 0.33479956659768995 0.6652004334023099;;; 0.09224249488323516 0.9077575051167648;;; 0.2603525256173366 0.7396474743826634;;; 0.02547786725176772 0.9745221327482323])\n\n\nSimulate some data.\n\nsd = DG.simulate(g, 20000, choicep, burnin=0, x0=1);\n\nEstimate and make the table.\n\nusing PrettyTables, DataFrames\nEu, _ = DGE.constructu(sd, g.β)\npretty_table(String,\n             DataFrame(\"x\"=&gt;statevec.(g.states),\n                       \"u\"=&gt;[g.u(1,[2,2],x) for x in g.states],\n                       \"û\"=&gt;Eu[1,2,:]),\n             backend=Val(:html))\n\n\"&lt;table&gt;\\n  &lt;thead&gt;\\n    &lt;tr class = \\\"header\\\"&gt;\\n      &lt;th style = \\\"text-align: right;\\\"&gt;x&lt;/th&gt;\\n      &lt;th style = \\\"text-align: right;\\\"&gt;u&lt;/th&gt;\\n      &lt;th style = \\\"text-align: right;\\\"&gt;û&lt;/th&gt;\\n    &lt;/tr&gt;\\n    &lt;tr class = \\\"subheader headerLastRow\\\"&gt;\\n      &lt;th style = \\\"text-align: rig\" ⋯ 1335 bytes ⋯ \"0.5&lt;/td&gt;\\n      &lt;td style = \\\"text-align: right;\\\"&gt;-0.495927&lt;/td&gt;\\n    &lt;/tr&gt;\\n    &lt;tr&gt;\\n      &lt;td style = \\\"text-align: right;\\\"&gt;Bool[1, 1, 1]&lt;/td&gt;\\n      &lt;td style = \\\"text-align: right;\\\"&gt;2.1&lt;/td&gt;\\n      &lt;td style = \\\"text-align: right;\\\"&gt;2.19592&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\\n\"\n\n\n\n\n\nFitted and True Choice Probabilities\nThe function DG.equlibrium returns a tuple consisting the output of NLsolve.solve and the equilibrium choice probabilities.\n\nres, choicep = DG.equilibrium(g)\n\nThe equilibrium choice probabilities are in a 3-dimensional array of size number of players by number of actions by number of states. choicep[i,a,s] is the probability player i chooses action a in state s.\n\n\n\n\n\n\nProblem 2\n\n\n\nCompare the true choice probabilities with the estimated choice probabilities from the model. To calculate the estimated choice probabilities, create a new DG.DynamicGame with the payoff function given by the estimated payoffs from problem 1. You may use the code below to get started. Create a table and/or figure that compares the estimated and true choice probabilities.\n\n\n\nfunction createufunction(Eu::AbstractArray)\n    N = size(Eu,1)\n    Nstates = size(Eu,3)\n    Nchoices = size(Eu,2)\n    Nexternal=Int(log2(Nstates))-N\n    states = BitVector.(digits.(0:(2^(N+Nexternal)-1), base=2, pad=N+Nexternal))\n    @show states\n    statedict = Dict(statevec(x)=&gt;x for x in 1:length(states))\n    u(i,a,x::Integer) = Eu[i,a[1],x]\n    u(i,a,s::AbstractVector) = u(i,a,statedict[s])\n    return(u)\nend\n\nû = createufunction(Eu) # assuming you used Eu as the estimated payoffs in Problem 1\nĝ = DG.DynamicGame(N,û, 0.9, Ex, 1:2, 1:ns)\nres, choicep̂ = DG.equilibrium(ĝ)\n\nstates = BitVector[[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 1]]\n\n\n(Results of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219]\n * Zero: [0.43955922437644823;;; 0.913176257672839;;; 0.6266624792046934;;; 2.275146214587601;;; 0.556765447824814;;; 2.273594097658097;;; 1.1069149968179584;;; 3.798763779247668]\n * Inf-norm of residuals: 0.000000\n * Iterations: 6\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 7\n * Jacobian Calls (df/dx): 6, [0.39184600227150584 0.6081539977284941;;; 0.2863503166590171 0.7136496833409829;;; 0.34826769515697936 0.6517323048430207;;; 0.09320236492088968 0.9067976350791103;;; 0.3642962024931178 0.6357037975068822;;; 0.0933336259965438 0.9066663740034562;;; 0.24844647576082812 0.7515535242391719;;; 0.02190774477145451 0.9780922552285455])\n\n\n\nusing AlgebraOfGraphics, CairoMakie\nlet\n  df = vcat(DataFrame(:state =&gt; 1:8, :P2 =&gt; choicep[1,2,:], :v =&gt; \"true\"),\n            DataFrame(:state =&gt; 1:8, :P2 =&gt; choicep̂[1,2,:], :v =&gt; \"estimated\"))\n  spec = data(df) *\n    mapping(:state =&gt; \"State\", :P2 =&gt; \"P(action 2|state)\", color=:v =&gt; \" \" )*\n    visual(Scatter)\n\n  draw(spec)\nend\n\n\n\n\n\n\n\n\nThe estimated choice probabilities are close to the true ones.\n\n\nCounterfactual Choice Probabilities\n\n\n\n\n\n\nProblem 3\n\n\n\nSuppose the payoff of action 2 in states 2, 4, 6, and 8 is decreased by 0.25. Compute the true and estimated change in choice probabilities. Compare the true and estimated change in choice probabilities in a figure or table.\n\n\nYou can create an appropriate shifted payoff function and new choice probabilities with the following code.\n\nu2(i,a,s) = u(i,a,s) + (s % 2 == 0)*(-0.25)\ng2 = DG.DynamicGame(N, u2, 0.9, Ex, 1:2, 1:ns)\nres2, choicep2 = DG.equilibrium(g2)\n\n(Results of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219]\n * Zero: [0.07260702240426554;;; 0.6726070224042463;;; 0.40090342379841165;;; 2.0009034237982544;;; 0.4009034237984133;;; 2.000903423798255;;; 0.7544589587376977;;; 3.3544589556452267]\n * Inf-norm of residuals: 0.000000\n * Iterations: 7\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 8\n * Jacobian Calls (df/dx): 8, [0.4818562145353414 0.5181437854646587;;; 0.3379133308792578 0.6620866691207421;;; 0.40109530197718407 0.5989046980228159;;; 0.11910810094530438 0.8808918990546957;;; 0.40109530197718374 0.5989046980228163;;; 0.11910810094530433 0.8808918990546958;;; 0.31985049327386983 0.6801495067261302;;; 0.03374945301899672 0.9662505469810033])\n\n\nNow we do the same for the estimates.\n\nû2(i,a,s) = û(i,a,s) + (s % 2 == 0)*(-0.25)\nĝ2 = DG.DynamicGame(N, û2, 0.9, Ex, 1:2, 1:ns)\nres2, choicep̂2 = DG.equilibrium(ĝ2)\n\n(Results of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219;;; 2.197224577336219]\n * Zero: [0.1645727502338893;;; 0.6381897835302784;;; 0.3406183572122029;;; 1.9891020925951197;;; 0.2688127129669349;;; 1.9856413628002245;;; 0.8164870235180358;;; 3.5083358070042197]\n * Inf-norm of residuals: 0.000000\n * Iterations: 6\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 7\n * Jacobian Calls (df/dx): 7, [0.4589494224490707 0.5410505775509293;;; 0.34565585585321523 0.6543441441467847;;; 0.415659278528858 0.584340721471142;;; 0.12035188911113155 0.8796481108888683;;; 0.4331985954564754 0.5668014045435247;;; 0.12071874890781799 0.8792812510921821;;; 0.30650987570477145 0.6934901242952286;;; 0.029075980147596357 0.9709240198524036])\n\n\nAnd plot the changes.\n\nfunction choicepchange(choicep, choicep2, choicep̂, choicep̂2)\n  df = vcat(DataFrame(:state =&gt; 1:8, :P2 =&gt; choicep2[1,2,:] - choicep[1,2,:], :v =&gt; \"true\"),\n            DataFrame(:state =&gt; 1:8, :P2 =&gt; choicep̂2[1,2,:] - choicep̂[1,2,:], :v =&gt; \"estimated\"))\n  spec = data(df) *\n    mapping(:state =&gt; \"State\", :P2 =&gt; \"ΔP(action 2|state)\", color=:v =&gt; \" \" )*\n    visual(Scatter)\n  return(spec)\nend\n\nspec = choicepchange(choicep, choicep2, choicep̂, choicep̂2)\ndraw(spec)\n\n\n\n\n\n\n\n\nThe model correctly estimates the counterfactual change in choice probabilities when shifting the payoff function.\n\n\nIncorrect Payoff Normalization\n\n\n\n\n\n\nProblem 4\n\n\n\nThe estimation code assumes the payoff of action 1 is 0 in all states. What if this assumption is incorrect? To explore what happens, simulate data where the payoff of action 1 is -(s-3.5)/5*(s % 2==1) in state s, and the payoff of action 2 is the same as in problems 1-3. Then estimate the model assuming the payoff of action 1 is 0. Finally, calculate the change in conditional choice probabilities from decreasing the payoff of action 2 in states 2, 4, 6, and 8 by 0.25 as in problem 3. Does an incorrect normalization affect the estimated change in choice probabilities?\n\n\nThis function will simulate the data, estimate, and compute counterfactuals.\n\nfunction counterfactual_choice_sim(u, uchange, Ex0, Ex1)\n  # baseline eq\n  g = DG.DynamicGame(N, u, 0.9, Ex0, 1:2, 1:ns)\n  res, choicep = DG.equilibrium(g)\n  # counterfactual eq\n  g2 = DG.DynamicGame(N,uchange(u), 0.9, Ex1, 1:2, 1:ns)\n  res, choicep2 = DG.equilibrium(g2)\n\n  # simulate\n  sd = DG.simulate(g, 20000, choicep, burnin=0, x0=1);\n\n  # estimate\n  Eu, _ = DGE.constructu(sd, g.β)\n  û = createufunction(Eu)\n  ĝ = DG.DynamicGame(N,û, 0.9, Ex0, 1:2, 1:ns)\n  res, choicep̂ = DG.equilibrium(ĝ)\n\n  # counterfactual estimate\n  ĝ2 = DG.DynamicGame(N,uchange(û), 0.9, Ex1, 1:2, 1:ns)\n  res, choicep̂2 = DG.equilibrium(ĝ2)\n\n  return(P0=choicep, P1=choicep2, P0̂ = choicep̂, P1̂=choicep̂2)\nend\n\nunew(i,a,s) = u(i,a,s) + -(a[1]==1)*(s-3.5)/5*(s % 2 == 1)\n\nP0, P1, P0̂, P1̂ = counterfactual_choice_sim(unew, u-&gt;((i,a,s)-&gt;u(i,a,s) + (s % 2==0)*(-0.25)), Ex, Ex)\n\nstates = BitVector[[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 1]]\n\n\n(P0 = [0.5610060924402981 0.4389939075597019;;; 0.2984365976346879 0.7015634023653121;;; 0.36514006529792875 0.6348599347020714;;; 0.09508038896389218 0.9049196110361079;;; 0.26844923909036 0.73155076090964;;; 0.09091572951119128 0.9090842704888087;;; 0.14220084553410992 0.8577991544658901;;; 0.024194699128239072 0.975805300871761], P1 = [0.6301498146360606 0.36985018536393943;;; 0.36189691785274003 0.63810308214726;;; 0.43398219822429973 0.5660178017757002;;; 0.12285990352298301 0.877140096477017;;; 0.32611119871633554 0.6738888012836645;;; 0.1165178414751213 0.8834821585248785;;; 0.17886092263659803 0.821139077363402;;; 0.031551201018643164 0.9684487989813568], P0̂ = [0.5657531721497557 0.43424682785024427;;; 0.3173619576176091 0.6826380423823909;;; 0.34423617875918705 0.6557638212408129;;; 0.10793984186960147 0.8920601581303985;;; 0.2660020253179698 0.7339979746820302;;; 0.08663263654400406 0.9133673634559959;;; 0.10474192447050808 0.8952580755294919;;; 0.026291902545951273 0.9737080974540487], P1̂ = [0.633203626429966 0.36679637357003403;;; 0.3811944649933754 0.6188055350066246;;; 0.40945045883083836 0.5905495411691616;;; 0.13779518567199184 0.8622048143280081;;; 0.3223280050907166 0.6776719949092834;;; 0.11070569745006503 0.8892943025499349;;; 0.13224898011603156 0.8677510198839684;;; 0.033978554184672966 0.966021445815327])\n\n\nAnd plot the changes.\n\nchoicepchange(P0, P1, P0̂, P1̂) |&gt; draw\n\n\n\n\n\n\n\n\nEven with an incorrect normalization, it appears that this method correctly estimates counterfactual changes in conditional choice probabiliites for changes in the payoff function.\n\n\nShift in Transitions\n\n\n\n\n\n\nProblem 5\n\n\n\nRepeat the analysis in problem 4, but instead of a shift in payoffs, suppose the transition probability of the exogenous state changes. Consider a change of Ex with pstay=0.7, to pstay=0.9. Comment on your findings.\n\n\n\nEx1(a, x::Integer) = Ex1(a, statevec(x))\npstay = 0.9 # each binary external state stays the same with probability pstay\nN = 1\nNexternal=2\nfunction Ex1(a, s::AbstractVector)\n  E = zeros(length(states))\n  sn = copy(s)\n  sn[1:N] .= a.-1\n  for j in 0:(2^Nexternal-1)\n    sn[(N+1):end] .= digits(j, base=2, pad=Nexternal)\n    i = stateind(sn)\n    nsame = sum(sn[(N+1):end].==s[(N+1):end])\n    E[i] = pstay^(nsame)*(1-pstay)^(Nexternal-nsame)\n  end\n  return(E)\nend\n\nP0, P1, P0̂, P1̂ = counterfactual_choice_sim(unew, u-&gt;u, Ex, Ex1)\nchoicepchange(P0, P1, P0̂, P1̂) |&gt; draw\n\nstates = BitVector[[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 1]]\n\n\n\n\n\n\n\n\n\nNow, the estimated changes in choice probabilities are incorrect. An incorrect payoff normalization does not mess up counterfactuals with respect to shifts in the payoff function, but does mess up counterfactuals with respect to changes in the transition probabilities.\n\n\nImplications\n\n\n\n\n\n\nProblem 6\n\n\n\nRead Kalouptsidi, Scott, and Souza-Rodrigues (2021). What findings of theirs do the above simulations illustrate?\n\n\nFor further reading, consider looking at Kalouptsidi, Scott, and Souza-Rodrigues (2017) and Kalouptsidi et al. (2024).\n\n\n\n\n\nReferences\n\nKalouptsidi, Myrto, Yuichi Kitamura, Lucas Lima, and Eduardo Souza-Rodrigues. 2024. “Counterfactual Analysis for Structural Dynamic Discrete Choice Models.” https://kitamura.sites.yale.edu/sites/default/files/files/Dynamic-Partial-ID-20240318.pdf.\n\n\nKalouptsidi, Myrto, Paul T. Scott, and Eduardo Souza-Rodrigues. 2017. “On the Non-Identification of Counterfactuals in Dynamic Discrete Games.” International Journal of Industrial Organization 50: 362–71. https://doi.org/https://doi.org/10.1016/j.ijindorg.2016.02.003.\n\n\n———. 2021. “Identification of Counterfactuals in Dynamic Discrete Choice Models.” Quantitative Economics 12 (2): 351–403. https://doi.org/https://doi.org/10.3982/QE1253."
  },
  {
    "objectID": "docs/dynamicid.html",
    "href": "docs/dynamicid.html",
    "title": "Identification in Dynamic Discrete Models",
    "section": "",
    "text": "Please write your answers in a literate programming format, such as a Pluto, Jupyter, or Quarto notebook. Turn in both the notebook file and an html or pdf.\n\nSimulation and Estimation\n\n\n\n\n\n\nProblem 1\n\n\n\nAdapt the equilibrium calculation, simulation, and estimation code in dyanmicgame.jl to compute an equlibrium, simulate, and estimate with a single agent firm (N=1). Most of the code will just work, except the transition function, but it is not needed here. Check your code by simulating some data with N=1 and Nexternal=2, and then estimating the model with the simulated data. When simulating, set T to 20_000 so that it easier to distinguish estimation noise from some other problem. Make a table similar to one in dynamicgame.jl at the end of the “Estimation” section comparing the true payoffs and estimated payoffs.\n\n\n\n\nFitted and True Choice Probabilities\nThe function DG.equlibrium returns a tuple consisting the output of NLsolve.solve and the equilibrium choice probabilities.\n\nres, choicep = DG.equilibrium(g)\n\nThe equilibrium choice probabilities are in a 3-dimensional array of size number of players by number of actions by number of states. choicep[i,a,s] is the probability player i chooses action a in state s.\n\n\n\n\n\n\nProblem 2\n\n\n\nCompare the true choice probabilities with the estimated choice probabilities from the model. To calculate the estimated choice probabilities, create a new DG.DynamicGame with the payoff function given by the estimated payoffs from problem 1. You may use the code below to get started. Create a table and/or figure that compares the estimated and true choice probabilities.\n\n\n\nfunction createufunction(Eu::AbstractArray)\n    N = size(Eu,1)\n    Nstates = size(Eu,3)\n    Nchoices = size(Eu,2)\n    Nexternal=Int(log2(Nstates))-N\n    states = BitVector.(digits.(0:(2^(N+Nexternal)-1), base=2, pad=N+Nexternal))\n    @show states\n    statedict = Dict(statevec(x)=&gt;x for x in 1:length(states))\n    u(i,a,x::Integer) = Eu[i,a[1],x]\n    u(i,a,s::AbstractVector) = u(i,a,statedict[s])\n    return(u)\nend\n\nû = createufunction(Eu) # assuming you used Eu as the estimated payoffs in Problem 1\nĝ = DG.DynamicGame(N,û, 0.9, Ex, 1:2, 1:ns)\nres, choicep̂ = DG.equilibrium(ĝ)\n\n# create table and/or figure comparing choicep̂ and choicep\n\n\n\nCounterfactual Choice Probabilities\n\n\n\n\n\n\nProblem 3\n\n\n\nSuppose the payoff of action 2 in states 2, 4, 6, and 8 is decreased by 0.25. Compute the true and estimated change in choice probabilities. Compare the true and estimated change in choice probabilities in a figure or table.\n\n\nYou can create an appropriate shifted payoff function and new choice probabilities with the following code.\n\nu2(i,a,s) = u(i,a,s) + (s % 2 == 0)*(-0.25)\ng2 = DG.DynamicGame(N, u2, 0.9, Ex, 1:2, 1:ns)\nres2, choicep2 = DG.equilibrium(g2)\n\n\n\nIncorrect Payoff Normalization\n\n\n\n\n\n\nProblem 4\n\n\n\nThe estimation code assumes the payoff of action 1 is 0 in all states. What if this assumption is incorrect? To explore what happens, simulate data where the payoff of action 1 is -(s-3.5)/5*(s % 2==1) in state s, and the payoff of action 2 is the same as in problems 1-3. Then estimate the model assuming the payoff of action 1 is 0. Finally, calculate the change in conditional choice probabilities from decreasing the payoff of action 2 in states 2, 4, 6, and 8 by 0.25 as in problem 3. Does an incorrect normalization affect the estimated change in choice probabilities?\n\n\n\n\nShift in Transitions\n\n\n\n\n\n\nProblem 5\n\n\n\nRepeat the analysis in problem 4, but instead of a shift in payoffs, suppose the transition probability of the exogenous state changes. Consider a change of Ex with pstay=0.7, to pstay=0.9. Comment on your findings.\n\n\n\n\nImplications\n\n\n\n\n\n\nProblem 6\n\n\n\nRead Kalouptsidi, Scott, and Souza-Rodrigues (2021). What findings of theirs do the above simulations illustrate?\n\n\nFor further reading, consider looking at Kalouptsidi, Scott, and Souza-Rodrigues (2017) and Kalouptsidi et al. (2024).\n\n\n\n\n\nReferences\n\nKalouptsidi, Myrto, Yuichi Kitamura, Lucas Lima, and Eduardo Souza-Rodrigues. 2024. “Counterfactual Analysis for Structural Dynamic Discrete Choice Models.” https://kitamura.sites.yale.edu/sites/default/files/files/Dynamic-Partial-ID-20240318.pdf.\n\n\nKalouptsidi, Myrto, Paul T. Scott, and Eduardo Souza-Rodrigues. 2017. “On the Non-Identification of Counterfactuals in Dynamic Discrete Games.” International Journal of Industrial Organization 50: 362–71. https://doi.org/https://doi.org/10.1016/j.ijindorg.2016.02.003.\n\n\n———. 2021. “Identification of Counterfactuals in Dynamic Discrete Choice Models.” Quantitative Economics 12 (2): 351–403. https://doi.org/https://doi.org/10.3982/QE1253."
  }
]