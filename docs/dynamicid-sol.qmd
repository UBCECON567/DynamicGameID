---
author: "Paul Schrimpf"
title: "Identification in Dynamic Discrete Models: Solutions"
subtitle: "UBC ECON 567 Assignment"
date: "March 28, 2024"
bibliography: "ddg.bib"
engine: julia
execute:
  daemon: 3600
format:
  html:
    toc: true
    html-math-method: katex
    theme: simplex
---



# Simulation and Estimation

::: {.callout-caution}

## Problem 1
Adapt the equilibrium calculation, simulation, and estimation code in [`dyanmicgame.jl`](https://faculty.arts.ubc.ca/pschrimpf/567/dynamicgame.html) to compute an equlibrium, simulate, and estimate with a single agent firm (`N=1`).  Most of the code will just work, except the `transition` function, but it is not needed here.  Check your code by simulating some data with `N=1` and `Nexternal=2`, and then estimating the model with the simulated data. Make a table similar to one in `dynamicgame.jl` at the end of the "Estimation" section comparing the true payoffs and estimated payoffs.

:::

### Equlibrium and Simulation Code

This is unmodifed from the provided Pluto notebook.

```{julia}
module DG

using NLsolve, LinearAlgebra, Distributions

export DynamicGame, equilibrium, Λ, vᵖ

struct DynamicGame{I<:Integer, U<:Function, R<:Real, E<:Function,A,S}
	"N is the number of players"
	N::I

	"""
	u(i, a, x) is the flow payoff function, where
	- i is a player index
	- a is a vector of actions of length N
	- x is a state
	"""
	u::U

	"Discount factor"
	β::R

	"""
	A function returning a vector with Ex(a,x)[x̃] = P(x̃|a,x)
	"""
	Ex::E

	"""
	Set of actions, must be 1:A or similar.
	"""
	actions::A

	"""
	Set of states, must be 1:S or similar.
	"""
	states::S
end

function pmax(v::AbstractVector)
	m = maximum(v)
	p = exp.(v .- m)
	p ./= sum(p)
	return(p)
end

function emax(v::AbstractVector)
	m = maximum(v)
	return(Base.MathConstants.γ + m + log(sum(exp.(v .- m))))
end

"""
   vᵖ(g::DynamicGame, P)

Computes choice specific value functions given strategies P.

P should be an array with P[i,a,x] = P(aᵢ=a| x)
"""
function vᵖ(g::DynamicGame, P)
	v = similar(P)
	a_all = Vector{eltype(g.actions)}(undef, g.N)
	E = zeros(eltype(P), length(g.actions), length(g.states),
		length(g.actions), length(g.states))
	y = zeros(eltype(v), length(g.actions), length(g.states))
	for i in 1:g.N
		y .= zero(eltype(y))
		E .= zero(eltype(E))
		for a0 in g.actions
			a_all[i] = a0
			for x in g.states
				for ami in Iterators.product(ntuple(i->g.actions, g.N-1)...)
					Pmi = one(eltype(P))
					for j in 1:g.N
						if j ≠ i
							k = j < i ? j : (j-1)
							Pmi *= P[j, ami[k], x]
							a_all[j] = ami[k]
						end
					end
					y[a0,x] += Pmi*g.u(i,a_all,x)
					for an in g.actions
						E[a0,x,an,:] .+= Pmi*g.Ex(a_all,x).*P[i,an,:]
						y[a0,x] +=  Pmi*g.β * (g.Ex(a_all,x).*P[i,an,:])'*
							(-log.(P[i,an,:]) .+ Base.MathConstants.γ)
					end
				end # ami
			end # x
		end # an
		SA = length(g.states)*length(g.actions)
		v[i, :, : ] .= reshape( (I - g.β*reshape(E, SA, SA)) \ reshape(y, SA),
							   length(g.actions), length(g.states))
	end # i
	return(v)
end

"""
	V̄(g::DynamicGame, vᵖ)

Returns the value function for game `g` with choice specific value functions vᵖ.
"""
function V̄(g::DynamicGame, vᵖ)
	return([emax(vᵖ[i,:,x]) for i in 1:g.N, x in g.states])
end

"""
    Λ(g::DynamicGame, vᵖ)

Computes best response choice probabilities given choice specific value function.
"""
function Λ(g::DynamicGame, vᵖ)
	p = similar(vᵖ)
    for (i,x) in Iterators.product(1:g.N, g.states)
		p[i,:,x] .= pmax(vᵖ[i,:,x])
	end
	return(p)
end


"""
    equilibrium(g::DynamicGame)

Compute equilibriumn choice probabilites of game `g`.

Returns a tuple `(out, P)` where `out` is the return value of `nlsolve`, and the choice probabilities are `P`.
"""
function equilibrium(g::DynamicGame)
	p = zeros(g.N,length(g.actions), length(g.states))
	#p = rand(size(p)...)
	#p .= 1/length(g.actions)
	p[:,1,:] .= 0.1
	p[:,2,:] .= 0.9
	function probs(z)
        p = similar(z, size(z,1), size(z,2) + 1, size(z,3))
		ez = exp.(z)
		for i in 1:size(p,1)
			for x in 1:size(p,3)
				se = sum(ez[i,:,x])
				p[i,2:end,x] .= ez[i,:,x]./(1 + se)
				p[i,1,x] = 1/(1 + se)
			end
		end
		return(p)
	end
	z = log.(p[:,2:end,:])
	for c in 1:size(z,2)
		z[:,c,:] .-= log.(p[:,1,:])
	end
	function eq!(e,z)
		p = probs(z)
		e .= (p - Λ(g, vᵖ(g, p)))[:,2:end,:]
		return(e)
	end
	out = nlsolve(eq!, z, autodiff=:forward, method=:trust_region)
	return(out, probs(out.zero))
end

"""
    simulate(g::DynamicGame, T, P; burnin=T, x0=rand(g.states))

Simulates game `g` for `T` periods with strategies `P`. Begins from state `x0` and discards the first `burnin + 1` periods.
"""
function simulate(g::DynamicGame, T, P; burnin=T, x0=rand(g.states))
	A = similar(g.actions,g.N,T)
	U = zeros(g.N,T)
	EV = copy(U)
	V = copy(U)
	X = similar(g.states,T)
	x = copy(x0)
	a = similar(g.actions, g.N)
	v = vᵖ(g, P)
	for t=-burnin:T
		ϵ = rand(Gumbel(0,1),g.N,length(g.actions))
		for i in 1:g.N
			#(_, aold) = findmax(v[i,:,x] + ϵ[i,:])
			(_, a[i]) = findmax(log.(P[i,:,x]).-log(P[i,1,x]) + ϵ[i,:])
			#if (i < 10)
			#	a[i]==aold || error("uh oh")
			#end
		end
		if (t>0)
			A[:,t] .= a
			X[t] = x
			for i in 1:g.N
				u = g.u(i,a,x)
				U[i,t] = u + ϵ[i,a[i]]
				V[i,t] = v[i,a[i],x] + ϵ[i,a[i]]
				EV[i,t] =v[i,a[i],x] - log.(P[i,a[i],x]) + Base.MathConstants.γ
			end
		end
		x = rand(DiscreteNonParametric(g.states, g.Ex(a,x)))
	end
	return(a=A, x=X, u=U, v=V, ev=EV)
end

end
```

### Estimation Code

```{julia}
"Dynamic game estimation."
module DGE

using LinearAlgebra,  Statistics, Distributions

function choiceprob(data)
	states = sort(unique(data.x))
	actions = sort(unique(data.a))
	N = size(data.a,1)
	P = [sum( (data.a[i,:].==a) .& (data.x.==x)) /
			sum(data.x.==x) for i ∈ 1:N, a ∈ actions, x ∈ states]
	P[isnan.(P)] .= 1/length(actions)
	return P
end

function transitioni(data)
	# P(x'|a_i,x,i)
	states = sort(unique(data.x))
	actions = sort(unique(data.a))
	N  = size(data.a,1)
	Pxi = [sum((data.x[2:end].==x̃) .& (data.x[1:(end-1)].==x)
			   .& (data.a[i,1:(end-1)].==a)) /
		   sum((data.x[1:(end-1)].==x) .& (data.a[i,1:(end-1)].==a) )
		   for i ∈ 1:N, x̃ ∈ states, a ∈ actions, x ∈ states]
	Pxi[isnan.(Pxi)] .= 1/length(states)
	return(Pxi)
end

function constructu(data, β; P=choiceprob(data), Pxi=transitioni(data), a0=1)
	states = sort(unique(data.x))
	actions = sort(unique(data.a))
	N  = size(data.a,1)
	Eu = similar(P)
	Eu[:,a0,:] .= 0
	v = similar(P)

	# recover v[:,a0,:]
	for i ∈ 1:N
		q = [Base.MathConstants.γ - log(P[i,a0,x]) for x ∈ states]
		E = Pxi[i,:,a0,:]'
		y = Eu[i,a0,:] + β*E*q
		v[i,a0,:] .= (I - β*E) \ y

		for a ∈ actions
			if a ≠ a0
				v[i,a,:] .= log.(P[i,a,:]) .- log.(P[i,a0,:]) .+ v[i,a0,:]
			end
		end
		# recover E[u(a[i],a[-i],x)|a[i],x]
		q = [Base.MathConstants.γ + log(sum(exp.(v[i,:,x]))) for x ∈ states]
		for a ∈ actions
			E = Pxi[i,:,a,:]'
			Eu[i,a,:] .= v[i,a,:] .- β*E*q
		end
	end
	return(Eu=Eu, v=v)
end


function transition(data)
	states = sort(unique(data.x))
	actions = sort(unique(data.a))
	N  = size(data.a,1)
	if N==1
        @info "N=1"
        Px = [sum( (data.x[2:end] .== x̃) .&
			(data.a[1,1:(end-1)].==a1) .&
			(data.x[1:(end-1)].==x)) /
		    sum((data.a[1,1:(end-1)].==a1) .&
			(data.x[1:(end-1)].==x)) for x̃ ∈ states,
			    a1 ∈ actions, x ∈ states ]
    elseif N==2
	    Px = [sum( (data.x[2:end] .== x̃) .&
			(data.a[1,1:(end-1)].==a1) .&
			(data.a[2,1:(end-1)].==a2) .&
			(data.x[1:(end-1)].==x)) /
		    sum((data.a[1,1:(end-1)].==a1) .&
		    (data.a[2,1:(end-1)].==a2) .&
			(data.x[1:(end-1)].==x)) for x̃ ∈ states,
			    a1 ∈ actions, a2 ∈ actions, x ∈ states ]
    else
        error("transition assumes 1 or 2 players")
    end
    Px[isnan.(Px)] .= 1 ./ length(states)
	return(Px)
end


function markovbootstrap(data, P = choiceprobs(data), Px=transition(data))
    states = sort(unique(data.x))
	actions = sort(unique(data.a))
	N  = size(data.a,1)
	bd = deepcopy(data)
	T = length(data.x)
	for t ∈ 2:T
		bd.x[t] = rand(DiscreteNonParametric(states, Px[:,bd.a[:,t-1]...,bd.x[t-1]]))
		for i in 1:N
			bd.a[i,t] = rand(DiscreteNonParametric(actions,P[i,:,bd.x[t]]))
		end
	end
	return(bd)
end

end
```

### Setup and Equlibrium Computation

Define the payoff function and transition process for the external states.
```{julia}
(N, ns, u, Ex, statevec, stateind, states) = let
	N = 1
	Nexternal=2

	# There's often some tedious book keeping involved in going from an integer state index to a vector representation of a state
	states = BitVector.(digits.(0:(2^(N+Nexternal)-1), base=2, pad=N+Nexternal))
	statevec(x::Integer)=states[x]
	stateind(s::AbstractVector)=findfirst([s==st for st in states])
	u(i, a, x::Integer) = u(i,a,statevec(x))
 	function u(i, a, s::AbstractVector)
		return( (a[i]-1)*s[i]*(3-Nexternal/2 + sum(s[(N+1):end]) - sum(s[1:N]))
			- 0.5*(a[i]-1)*(1-s[i]) # entry cost
			- s[i]*(0.8 + 0.1*sum(s[1:N]))*(a[i]-1)) # fixed cost
	end

	Ex(a, x::Integer) = Ex(a, statevec(x))
	pstay = 0.7 # each binary external state stays the same with probability pstay
    function Ex(a, s::AbstractVector)
		E = zeros(length(states))
		sn = copy(s)
		sn[1:N] .= a.-1
		for j in 0:(2^Nexternal-1)
			sn[(N+1):end] .= digits(j, base=2, pad=Nexternal)
			i = stateind(sn)
			nsame = sum(sn[(N+1):end].==s[(N+1):end])
			E[i] = pstay^(nsame)*(1-pstay)^(Nexternal-nsame)
		end
		return(E)
	end
	N, length(states), u, Ex, statevec, stateind, states
end
```

Compute the equilibrium.

```{julia}
g = DG.DynamicGame(N, u, 0.9, Ex, 1:2, 1:ns)
res, choicep = DG.equilibrium(g)
```

Simulate some data.
```{julia}
sd = DG.simulate(g, 20000, choicep, burnin=0, x0=1);
```

Estimate and make the table.
```{julia}
using PrettyTables, DataFrames
Eu, _ = DGE.constructu(sd, g.β)
pretty_table(String,
		     DataFrame("x"=>statevec.(g.states),
				       "u"=>[g.u(1,[2,2],x) for x in g.states],
				       "û"=>Eu[1,2,:]),
		     backend=Val(:html))
```

# Fitted and True Choice Probabilities

The function `DG.equlibrium` returns a tuple consisting the output of `NLsolve.solve` and the equilibrium choice probabilities.
```{julia}
#| eval: false
res, choicep = DG.equilibrium(g)
```
The equilibrium choice probabilities are in a 3-dimensional array of size number of players by number of actions by number of states. `choicep[i,a,s]` is the probability player `i` chooses action `a` in state `s`.

::: {.callout-caution}

## Problem 2

Compare the true choice probabilities with the estimated choice probabilities from the model. To calculate the estimated choice probabilities, create a new `DG.DynamicGame` with the payoff function given by the estimated payoffs from problem 1. You may use the code below to get started. Create a table and/or figure that compares the estimated and true choice probabilities.

:::

```{julia}
function createufunction(Eu::AbstractArray)
	N = size(Eu,1)
    Nstates = size(Eu,3)
    Nchoices = size(Eu,2)
	Nexternal=Int(log2(Nstates))-N
	states = BitVector.(digits.(0:(2^(N+Nexternal)-1), base=2, pad=N+Nexternal))
    @show states
    statedict = Dict(statevec(x)=>x for x in 1:length(states))
    u(i,a,x::Integer) = Eu[i,a[1],x]
    u(i,a,s::AbstractVector) = u(i,a,statedict[s])
    return(u)
end

û = createufunction(Eu) # assuming you used Eu as the estimated payoffs in Problem 1
ĝ = DG.DynamicGame(N,û, 0.9, Ex, 1:2, 1:ns)
res, choicep̂ = DG.equilibrium(ĝ)
```

```{julia}
using AlgebraOfGraphics, CairoMakie
let
  df = vcat(DataFrame(:state => 1:8, :P2 => choicep[1,2,:], :v => "true"),
            DataFrame(:state => 1:8, :P2 => choicep̂[1,2,:], :v => "estimated"))
  spec = data(df) *
    mapping(:state => "State", :P2 => "P(action 2|state)", color=:v => " " )*
    visual(Scatter)

  draw(spec)
end
```

The estimated choice probabilities are close to the true ones.


# Counterfactual Choice Probabilities


::: {.callout-caution}

## Problem 3
Suppose the payoff of action 2 in states 2, 4, 6, and 8 is decreased by 0.25. Compute the true and estimated change in choice probabilities. Compare the true and estimated change in choice probabilities in a figure or table.

:::

You can create an appropriate shifted payoff function and new choice probabilities with the following code.

```{julia}
u2(i,a,s) = u(i,a,s) + (s % 2 == 0)*(-0.25)
g2 = DG.DynamicGame(N, u2, 0.9, Ex, 1:2, 1:ns)
res2, choicep2 = DG.equilibrium(g2)
```

Now we do the same for the estimates.

```{julia}
û2(i,a,s) = û(i,a,s) + (s % 2 == 0)*(-0.25)
ĝ2 = DG.DynamicGame(N, û2, 0.9, Ex, 1:2, 1:ns)
res2, choicep̂2 = DG.equilibrium(ĝ2)
```

And plot the changes.
```{julia}
function choicepchange(choicep, choicep2, choicep̂, choicep̂2)
  df = vcat(DataFrame(:state => 1:8, :P2 => choicep2[1,2,:] - choicep[1,2,:], :v => "true"),
            DataFrame(:state => 1:8, :P2 => choicep̂2[1,2,:] - choicep̂[1,2,:], :v => "estimated"))
  spec = data(df) *
    mapping(:state => "State", :P2 => "ΔP(action 2|state)", color=:v => " " )*
    visual(Scatter)
  return(spec)
end

spec = choicepchange(choicep, choicep2, choicep̂, choicep̂2)
draw(spec)
```

The model correctly estimates the counterfactual change in choice probabilities when shifting the payoff function.


# Incorrect Payoff Normalization

::: {.callout-caution}

## Problem 4

The estimation code assumes the payoff of action 1 is 0 in all states. What if this assumption is incorrect? To explore what happens, simulate data where the payoff of action 1 is `-(s-3.5)/5*(s % 2==1)` in state `s`, and the payoff of action 2 is the same as in problems 1-3. Then estimate the model assuming the payoff of action 1 is 0. Finally, calculate the change in conditional choice probabilities from decreasing the payoff of action 2 in states 2, 4, 6, and 8 by 0.25 as in problem 3. Does an incorrect normalization affect the estimated change in choice probabilities?

:::


This function will simulate the data, estimate, and compute counterfactuals.
```{julia}
function counterfactual_choice_sim(u, uchange, Ex0, Ex1)
  # baseline eq
  g = DG.DynamicGame(N, u, 0.9, Ex0, 1:2, 1:ns)
  res, choicep = DG.equilibrium(g)
  # counterfactual eq
  g2 = DG.DynamicGame(N,uchange(u), 0.9, Ex1, 1:2, 1:ns)
  res, choicep2 = DG.equilibrium(g2)

  # simulate
  sd = DG.simulate(g, 20000, choicep, burnin=0, x0=1);

  # estimate
  Eu, _ = DGE.constructu(sd, g.β)
  û = createufunction(Eu)
  ĝ = DG.DynamicGame(N,û, 0.9, Ex0, 1:2, 1:ns)
  res, choicep̂ = DG.equilibrium(ĝ)

  # counterfactual estimate
  ĝ2 = DG.DynamicGame(N,uchange(û), 0.9, Ex1, 1:2, 1:ns)
  res, choicep̂2 = DG.equilibrium(ĝ2)

  return(P0=choicep, P1=choicep2, P0̂ = choicep̂, P1̂=choicep̂2)
end

unew(i,a,s) = u(i,a,s) + -(a[1]==1)*(s-3.5)/5*(s % 2 == 1)

P0, P1, P0̂, P1̂ = counterfactual_choice_sim(unew, u->((i,a,s)->u(i,a,s) + (s % 2==0)*(-0.25)), Ex, Ex)
```


And plot the changes.
```{julia}
choicepchange(P0, P1, P0̂, P1̂) |> draw
```

Even with an incorrect normalization, it appears that this method correctly estimates counterfactual changes in conditional choice probabiliites for changes in the payoff function.


# Shift in Transitions


::: {.callout-caution}

## Problem 5

Repeat the analysis in problem 4, but instead of a shift in payoffs, suppose the transition probability of the exogenous state changes. Consider a change of `Ex` with `pstay=0.7`, to `pstay=0.9`. Comment on your findings.

:::

```{julia}
Ex1(a, x::Integer) = Ex1(a, statevec(x))
pstay = 0.9 # each binary external state stays the same with probability pstay
N = 1
Nexternal=2
function Ex1(a, s::AbstractVector)
  E = zeros(length(states))
  sn = copy(s)
  sn[1:N] .= a.-1
  for j in 0:(2^Nexternal-1)
	sn[(N+1):end] .= digits(j, base=2, pad=Nexternal)
	i = stateind(sn)
	nsame = sum(sn[(N+1):end].==s[(N+1):end])
	E[i] = pstay^(nsame)*(1-pstay)^(Nexternal-nsame)
  end
  return(E)
end

P0, P1, P0̂, P1̂ = counterfactual_choice_sim(unew, u->u, Ex, Ex1)
choicepchange(P0, P1, P0̂, P1̂) |> draw
```

Now, the estimated changes in choice probabilities are incorrect. An incorrect payoff normalization does not mess up counterfactuals with respect to shifts in the payoff function, but does mess up counterfactuals with respect to changes in the transition probabilities.

# Implications

::: {.callout-caution}

## Problem 6

Read @kalouptsidi2021. What findings of theirs do the above simulations illustrate?

:::

For further reading, consider looking at @kalouptsidi2017 and @kalouptsidi2024.
